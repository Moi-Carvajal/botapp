# -*- coding: utf-8 -*-
"""Arquitectura.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nqJj1baUhL6-dmOU9hCAlRIClV5Panm2
"""

# try:
#   import transformers       #libreria transformers     
#   from transformers import BertModel, BertConfig, BertPreTrainedModel
# except ModuleNotFoundError:
#   !pip install transformers
#   import transformers                                   
#   from transformers import BertModel, BertConfig, BertPreTrainedModel

import transformers                                   
from transformers import BertModel, BertPreTrainedModel
import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss

class BertForSequenceClassification(BertPreTrainedModel):
    #en __init__ escribes las capas que vas a utilizar.
    def __init__(self, config, weight=None):
        super(BertForSequenceClassification, self).__init__(config)
        #Numero de etiquetas
        self.num_labels = config.num_labels
        #self.bert es literalmente todo el modelo de Bert pre entrenado
        self.bert = BertModel(config)
        #Capa de Dropout
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        #Capa Densa con shape (768, num_labels)
        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)
        self.weight = weight
        #Inicializamos los pesos
        self.init_weights()
    #El metodo forward es donde ocurre TODO, aqui tienes que escribir toda la 
    #logica de tu red neuronal.
    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        Phase=None,
    ):
        #Esta es la capa de Bert, osea que los datos primero van a pasar por aquí
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
        )

        # outputs es una tupla, solo tomamos el segundo valor porque ese es 
        #el output del modelo, si quieres ver toda la tupla checa este link:
        #https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        #Te vas a la parte de return y ahí te dice todo lo que esa capa te arroja

        pooled_output = outputs[1]

        #Despues ese output lo metes a las siguientes 2 capas
        #Esto es a lo que mucha gente se refiere como "Heads"
        #En este caso es una cabeza para clasificar
        
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        #Aqui solo estamos haciendo una tupla con los valores de salida como
        # outputs[0] y el resto de las salidas de la capa de bert.
        #Abajo hay un ejemplo de lo que esta linea esta haciendo.

        outputs = (logits,) + outputs[2:]
        #Aqui tenemos la función de perdida, normalmente esta afuera del modelo
        #pero se puede escribir aqui sin ningún problema

        if (Phase == 'train' or Phase == 'test'):
          loss_fct = CrossEntropyLoss(weight=self.weight)
          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
          outputs= (loss,) + outputs

        #EN ESTA ULTIMA PARTE EL MODELO REGRESARA LOS RESULTADOS
        return outputs  # (loss), logits, (hidden_states), (attentions)